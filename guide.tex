\documentclass[12pt]{article}
\usepackage{color}

\title{A Practical Guide to Support Vector Machines}
\author{Chih-Jen Lin, Chih-Chung Chang}

\begin{document} 
\maketitle

\section{Introduction}

SVM (Support Vector Machines) is a newer technique for data
classification and regression. However, people who are not 
familiar with SVM almost always get unsatisfactory result 
at first. Here we propose a "cookbook" approach which usually 
gives reasonable result.

\subsection{Real World Examples}

Here are some real world examples. These datasets are
tried first by our users, then our procedure is used
on these datasets. As can be seen from the following 
table, the procedure can achieve better performance.

\vspace{0.5cm}
\begin{tabular}{l|l|l}
Application area & Accuracy before & Accuracy after \\
\hline

astroparticle physics \footnotemark[1] & 75.2\% & 97.3\% \\

bioinformatics \footnotemark[2] & 36\% & 83.88\% \\

comparison with neural net \footnotemark[3] & 4.88\% & 87.8\% \\

\end{tabular}

\footnotetext[1]{Courtesy of Jan Conrad ({\tt conrad@unix.tsl.uu.se}).}
% /home/professor/cjlin/software/svm/svmguide/conrad
\footnotetext[2]{???}
\footnotetext[3]{Courtesy of Berend Kapelle ({\tt berend.kapelle@gmx.de}).}
% http://www.kernel-machines.org/blackboard/messages/860.html
% http://berend.gmxhome.de/train_n.dat
% http://berend.gmxhome.de/test_n.dat

\vspace{0.5cm}
These datasets are available in {\tt http://www.csie.ntu.edu.tw/\~\/cjlin/...}

\subsection{Proposed Procedure}

Many users are doing the following procedure now:
\begin{itemize}
\item Transform data to the format of an SVM software
\item Randomly try a few kernels and parameters
\item Test
\end{itemize}

We propose this procedure instead:
\begin{itemize}
\item Transform data to the format of an SVM software
\item Simple scaling on the data
\item Consider the RBF kernel $k(x,y)=e^{-\gamma|x-y|^2}$ 
\item Use cross-validation to find the best parameter
      $C$ and $\gamma$ systematically.
\item Test
\end{itemize}

We discuss them in detail in the following 
sections.

\section{Data Preprocessing}

A dataset consists of some data instances. Each
instance has one "target value" and a number of
"attributes." In classification, the target value 
is the class of the instance; in regression, 
the target is the function value.

The goal of SVM is to produce a model which 
predicts target value given the attributes.

\subsection{Categorical Feature}

SVM requires each instance of data to be represented
as a vector of real numbers. So if you have categorical
attributes in your data, you first have to convert 
them into numeric data. We recommend using N numbers to 
represent an N-category attribute. Only one of the N
numbers are 1, other are zero. For example if you have
a 3-category attribute, you can represent them as (0,0,1),
(0,1,0), and (1,0,0). In our experience this works
better than using a single number to represent a 
categorical attribute.

\subsection{Scaling}

Numeric attributes can be used as is, but
scaling them before applying SVM is very important.

example: heart, first 5 rows ... 
performance comparison.
refer to NN FAQ

The main reason is to avoid attributes which have 
greater numeric range dominate those have 
smaller numeric range. Another reason is to 
avoid numeric difficulty during the calculation. 
We recommend linearly scale each attribute to 
the range $[-1,+1]$ or $[0,1]$.

Of course you have to use the same method to 
scale test data before testing.

\section{Model Selection}

\subsection{RBF kernel}
There are many proposed kernel functions for 
SVM. In our experience the RBF kernel is the 
most useful one. Other common kernels include:

\begin{itemize}
\item linear: a special case of RBF [Keerthi and Lin 2003]
\item polynomial: easily cause numerical difficulties
$(<1)^d \rightarrow 0, (>1)^d \rightarrow \infty$
\item sigmoid:  for certain parameters, it behaves like RBF [Lin and Lin 2003]
\end{itemize}

\subsection{Grid Search}
There are two parameters need to be considered 
while using RBF kernel ---  $C$ and $\gamma$. 
It is not known beforehand  which $C$ and 
$\gamma$ values are best for one problem, 
so some kind of model selection 
(parameter search) must be done. 

We recommend doing a "grid-search" on $C$ and
$\gamma$. Basically pairs of $(C,\gamma)$ are tried 
and the cross-validation accuracy are compared to 
get the best $(C,\gamma)$. We found that trying
exponentially growing sequences of $C$ and $\gamma$
can give a good picture where good parameters lie.
(for example, $C=2^{-6},2^{-5},...,2^{14}$, 
$\gamma=2^{-14},2^{-13},...,2^4$)

figure.

Doing grid search is time-consuming, 
so we recommend using a coarse grid first. 
After identifying a "better" region on the
grid, a finer grid search on that region can 
be conducted.

figure.

The above approach works well for small-to-medium 
sized dataset. For larger dataset, the time for
doing a complete "grid-search" may be prohibitive.
A feasible approach is to randomly choose
a subset of the dataset, doing "grid-search" on
them, then using the best parameters to train
the complete dataset.

\section{Discussion}

Despite people usually claim SVM is free of 
"curse of dimensionality", it is still bad to 
have too many attributes which are not predictive, 
especially when the number of instance in the
data set are small. If you have thousands of
attributes, chances are you have to choose a 
subset of them before giving the data to SVM.
We don't have a good approach to feature 
selection yet.

\end{document}
